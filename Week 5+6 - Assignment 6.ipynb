{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 and 6 - Assignment 6\n",
    "#### Team 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Classify new \"test\" documents using already classified \"training\" documents and perform a sentiment analysis.\n",
    "\n",
    "For this project, the dataset used was from the CrowdFlower library (https://www.crowdflower.com/data-for-everyone/), using the \"Twitter sentiment analysis: Self-driving cars\" dataset. This dataset includes posts from Twitter regarding users' opinions of self-driving cars, the tweets' classification as relevant to the topic or not, and if relevant the classification of tweet sentiment as integers (from a positive 5 to a negative 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "\n",
    "#### Seeing what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>724227037</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.5367</td>\n",
       "      <td>Chicago metro expected to be fully autonomous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>724227038</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>I love the infotainment system in my new car. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id     sentiment  sentiment:confidence  \\\n",
       "0   724227031             5                0.7579   \n",
       "1   724227032             5                0.8775   \n",
       "2   724227033             2                0.6805   \n",
       "3   724227034             2                0.8820   \n",
       "4   724227035             3                1.0000   \n",
       "5   724227036             3                1.0000   \n",
       "6   724227037  not_relevant                0.5367   \n",
       "7   724227038  not_relevant                0.6548   \n",
       "8   724227039             5                0.7187   \n",
       "9   724227040             1                0.6412   \n",
       "10  724227041             3                0.9184   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "6   Chicago metro expected to be fully autonomous ...  \n",
       "7   I love the infotainment system in my new car. ...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.io.parsers.read_csv(\"https://raw.githubusercontent.com/Galanopoulog/DATA620-Assignment-6/master/Self-Driving-Cars.csv\")\n",
    "data = data.drop(data.columns[[1,2,3,4,7,8,9]], axis=1)\n",
    "data.text.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True) #remove ascii characters from text\n",
    "data.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7156"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count relevant and irrelevant\n",
    "rel_count = len(data[data.sentiment!=\"not_relevant\"])\n",
    "irrel_count = len(data[data.sentiment==\"not_relevant\"])\n",
    "neut_count = len(data[data.sentiment==\"3\"])\n",
    "\n",
    "print \"Relevant: %d\" %rel_count\n",
    "print \"Not_Relevant: %d\" %irrel_count\n",
    "print \"Neutral: %d\" %neut_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key things to note is that 1) the tweets and sentiment values are objects, which may need to be changed in the future for manipulations and 2) the number of non-relevant tweets is significantly smaller than the relevant portion, so splitting the data into training and test sets should probably be more methodical than just randomly shuffling the data. Also, it will be interesting to see the difference between the relevant neutral tweets and the irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Splitting the data\n",
    "\n",
    "The data is split into the training set, the evaluation set and the final set that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What sizes should our sets be?\n",
    "train_set = int(0.7 * len(data))\n",
    "dev_set = int(.15 * len(data))\n",
    "final_set = len(data) - train_set - dev_set\n",
    "Total = train_set + dev_set + final_set\n",
    "\n",
    "print \"Training set: %d\" %train_set\n",
    "print \"Evaluation set: %d\" %dev_set\n",
    "print \"Testing set: %d\" %final_set\n",
    "print \"Total: %d\" %Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's split our data into relevant and non-relevant parts.\n",
    "relevant = data[data.sentiment!=\"not_relevant\"]\n",
    "nonrelevant = data[data.sentiment==\"not_relevant\"]\n",
    "\n",
    "# Let's take 70%, 15% and the 15% of each\n",
    "rel_train = int(0.7 * len(relevant))\n",
    "rel_dev = int(.15 * len(relevant))\n",
    "rel_fin = len(relevant) - rel_train - rel_dev\n",
    "rel_Total = rel_train + rel_dev + rel_fin\n",
    "\n",
    "irrel_train = int(0.7 * len(nonrelevant))\n",
    "irrel_dev = int(.15 * len(nonrelevant))\n",
    "irrel_fin = len(nonrelevant) - irrel_train - irrel_dev\n",
    "irrel_Total = irrel_train + irrel_dev + irrel_fin\n",
    "\n",
    "print \"Training set: %d\" %rel_train\n",
    "print \"Evaluation set: %d\" %rel_dev\n",
    "print \"Testing set: %d\" %rel_fin\n",
    "print \"Total: %d\" %rel_Total\n",
    "print \"----------------------\"\n",
    "print \"Training set: %d\" %irrel_train\n",
    "print \"Evaluation set: %d\" %irrel_dev\n",
    "print \"Testing set: %d\" %irrel_fin\n",
    "print \"Total: %d\" %irrel_Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sets for relevant data\n",
    "rel_train = relevant[0:4860]\n",
    "rel_dev = relevant[4860:5901]\n",
    "rel_fin = relevant[5901:6943]\n",
    "\n",
    "# Make sets for non-relevant data\n",
    "irrel_train = nonrelevant[0:149]\n",
    "irrel_dev = nonrelevant[149:180]\n",
    "irrel_fin = nonrelevant[180:231]\n",
    "\n",
    "# Combine sets by set types\n",
    "train_set = pd.concat([rel_train, irrel_train], axis=0)\n",
    "dev_set = pd.concat([rel_dev, irrel_dev], axis=0)\n",
    "final_set = pd.concat([rel_fin, irrel_fin], axis=0)\n",
    "\n",
    "# Determine lengths and compare to set size estimates\n",
    "print \"Training set: %d\" %len(train_set)\n",
    "print \"Evaluation set: %d\" %len(dev_set)\n",
    "print \"Testing set: %d\" %len(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and Categorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = train_set[['text', 'sentiment']]\n",
    "tuples = [tuple(x) for x in subset.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "all_words = set(word.lower() for passage in tuples for word in word_tokenize(passage[0]))\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-216-29a15e077ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
