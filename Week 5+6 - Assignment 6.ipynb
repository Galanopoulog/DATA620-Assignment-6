{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 and 6 - Assignment 6\n",
    "#### Team 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Classify new \"test\" documents using already classified \"training\" documents.\n",
    "\n",
    "For this project, the dataset used was from the CrowdFlower library (https://www.crowdflower.com/data-for-everyone/), using the \"Twitter sentiment analysis: Self-driving cars\" dataset. This dataset includes posts from Twitter regarding users' opinions of self-driving cars, the tweets' classification as relevant to the topic or not, and if relevant the classification of tweet sentiment as integers (from a positive 5 to a negative 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "\n",
    "#### Seeing what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import sys\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>724227037</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.5367</td>\n",
       "      <td>Chicago metro expected to be fully autonomous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>724227038</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>I love the infotainment system in my new car. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id     sentiment  sentiment:confidence  \\\n",
       "0   724227031             5                0.7579   \n",
       "1   724227032             5                0.8775   \n",
       "2   724227033             2                0.6805   \n",
       "3   724227034             2                0.8820   \n",
       "4   724227035             3                1.0000   \n",
       "5   724227036             3                1.0000   \n",
       "6   724227037  not_relevant                0.5367   \n",
       "7   724227038  not_relevant                0.6548   \n",
       "8   724227039             5                0.7187   \n",
       "9   724227040             1                0.6412   \n",
       "10  724227041             3                0.9184   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "6   Chicago metro expected to be fully autonomous ...  \n",
       "7   I love the infotainment system in my new car. ...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.io.parsers.read_csv(\"https://raw.githubusercontent.com/Galanopoulog/DATA620-Assignment-6/master/Self-Driving-Cars.csv\")\n",
    "data = data.drop(data.columns[[1,2,3,4,7,8,9]], axis=1)\n",
    "data.text.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True) #remove ascii characters\n",
    "data.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_unit_id                  int64\n",
      "sentiment                object\n",
      "sentiment:confidence    float64\n",
      "text                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant: 6943\n",
      "Not_Relevant: 213\n",
      "Neutral: 4245\n"
     ]
    }
   ],
   "source": [
    "# Count relevant and irrelevant\n",
    "rel_count = len(data[data.sentiment!=\"not_relevant\"])\n",
    "irrel_count = len(data[data.sentiment==\"not_relevant\"])\n",
    "neut_count = len(data[data.sentiment==\"3\"])\n",
    "\n",
    "print \"Relevant: %d\" %rel_count\n",
    "print \"Not_Relevant: %d\" %irrel_count\n",
    "print \"Neutral: %d\" %neut_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key things to note is that 1) the tweets and sentiment values are objects, which may need to be changed in the future for manipulations and 2) the number of non-relevant tweets is significantly smaller than the relevant portion, so splitting the data into training and test sets should probably be more methodical than just randomly shuffling the data. Also, it will be interesting to see the difference between the relevant neutral tweets and the irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Splitting the data\n",
    "\n",
    "The data is split into the training set, the evaluation set and the final set that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 5009\n",
      "Evaluation set: 1073\n",
      "Testing set: 1074\n",
      "Total: 7156\n"
     ]
    }
   ],
   "source": [
    "# What sizes should our sets be?\n",
    "train_set = int(0.7 * len(data))\n",
    "dev_set = int(.15 * len(data))\n",
    "final_set = len(data) - train_set - dev_set\n",
    "Total = train_set + dev_set + final_set\n",
    "\n",
    "print \"Training set: %d\" %train_set\n",
    "print \"Evaluation set: %d\" %dev_set\n",
    "print \"Testing set: %d\" %final_set\n",
    "print \"Total: %d\" %Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4860\n",
      "Evaluation set: 1041\n",
      "Testing set: 1042\n",
      "Total: 6943\n",
      "----------------------\n",
      "Training set: 149\n",
      "Evaluation set: 31\n",
      "Testing set: 33\n",
      "Total: 213\n"
     ]
    }
   ],
   "source": [
    "# Let's split our data into relevant and non-relevant parts.\n",
    "relevant = data[data.sentiment!=\"not_relevant\"]\n",
    "nonrelevant = data[data.sentiment==\"not_relevant\"]\n",
    "\n",
    "# Let's take 70%, 15% and the 15% of each\n",
    "rel_train = int(0.7 * len(relevant))\n",
    "rel_dev = int(.15 * len(relevant))\n",
    "rel_fin = len(relevant) - rel_train - rel_dev\n",
    "rel_Total = rel_train + rel_dev + rel_fin\n",
    "\n",
    "irrel_train = int(0.7 * len(nonrelevant))\n",
    "irrel_dev = int(.15 * len(nonrelevant))\n",
    "irrel_fin = len(nonrelevant) - irrel_train - irrel_dev\n",
    "irrel_Total = irrel_train + irrel_dev + irrel_fin\n",
    "\n",
    "print \"Training set: %d\" %rel_train\n",
    "print \"Evaluation set: %d\" %rel_dev\n",
    "print \"Testing set: %d\" %rel_fin\n",
    "print \"Total: %d\" %rel_Total\n",
    "print \"----------------------\"\n",
    "print \"Training set: %d\" %irrel_train\n",
    "print \"Evaluation set: %d\" %irrel_dev\n",
    "print \"Testing set: %d\" %irrel_fin\n",
    "print \"Total: %d\" %irrel_Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 5009\n",
      "Evaluation set: 1072\n",
      "Testing set: 1075\n"
     ]
    }
   ],
   "source": [
    "# Make sets for relevant data\n",
    "rel_train = relevant[0:4860]\n",
    "rel_dev = relevant[4860:5901]\n",
    "rel_fin = relevant[5901:6943]\n",
    "\n",
    "# Make sets for non-relevant data\n",
    "irrel_train = nonrelevant[0:149]\n",
    "irrel_dev = nonrelevant[149:180]\n",
    "irrel_fin = nonrelevant[180:231]\n",
    "\n",
    "# Combine sets by set types\n",
    "train_set = pd.concat([rel_train, irrel_train], axis=0)\n",
    "dev_set = pd.concat([rel_dev, irrel_dev], axis=0)\n",
    "final_set = pd.concat([rel_fin, irrel_fin], axis=0)\n",
    "\n",
    "# Determine lengths and compare to set size estimates\n",
    "print \"Training set: %d\" %len(train_set)\n",
    "print \"Evaluation set: %d\" %len(dev_set)\n",
    "print \"Testing set: %d\" %len(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sets now have an evenly distributed number of relevant and non-relevant entries and match the 70%, 15%, 15% data distribution that was considered optimal originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and Categorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>724227610</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Audi is the first carmaker to get a license fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id sentiment  sentiment:confidence  \\\n",
       "0   724227031         5                0.7579   \n",
       "1   724227032         5                0.8775   \n",
       "2   724227033         2                0.6805   \n",
       "3   724227034         2                0.8820   \n",
       "4   724227035         3                1.0000   \n",
       "5   724227036         3                1.0000   \n",
       "8   724227039         5                0.7187   \n",
       "9   724227040         1                0.6412   \n",
       "10  724227041         3                0.9184   \n",
       "11  724227610         3                1.0000   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  \n",
       "11  Audi is the first carmaker to get a license fr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making tuples for classification\n",
    "subset = train_set[['text', 'sentiment']]\n",
    "tuples = [tuple(x) for x in subset.values]\n",
    "\n",
    "train_set[['text']][0:10]\n",
    "subset[['text']]\n",
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "all_words = toktok.tokenize(subset[['text']] )\n",
    "\n",
    "t = [({word: (word in (x[0])) for word in all_words}, x[1]) for x in tuples]\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    Damn = True                1 : 3      =     41.9 : 1.0\n",
      "                    Fuck = True                1 : 3      =     41.9 : 1.0\n",
      "                    cool = True                5 : 3      =     33.4 : 1.0\n",
      "                 Awesome = True                5 : 3      =     32.7 : 1.0\n",
      "                    seem = True                1 : 3      =     23.3 : 1.0\n",
      "                  reduce = True                5 : 3      =     22.4 : 1.0\n",
      "                       c = False          not_re : 2      =     20.9 : 1.0\n",
      "                tracking = True           not_re : 3      =     20.4 : 1.0\n",
      "         _______________ = True           not_re : 3      =     20.4 : 1.0\n",
      "                     Dor = True           not_re : 3      =     20.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifying Methods\n",
    "\n",
    "# Naive Bayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:\n",
      "------------------\n",
      "Naive Bayes: 50\n",
      "MNB: 58\n",
      "BernoulliNB: 51\n",
      "LogisticRegression: 61\n",
      "SGDClassifier: 37\n",
      "LinearSVC: 62\n"
     ]
    }
   ],
   "source": [
    "# Other Methods to Consider\n",
    "\n",
    "# MNB\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(t)\n",
    "\n",
    "# BernoulliNB\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(t)\n",
    "\n",
    "# Logistic Regression\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(t)\n",
    "\n",
    "# SGDC\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(t)\n",
    "\n",
    "# Linear SVC\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(t)\n",
    "\n",
    "print \"Percent Accuracy:\"\n",
    "print \"------------------\"\n",
    "print \"Naive Bayes: %d\" %round((nltk.classify.accuracy(classifier, testing_set))*100, 2)\n",
    "print \"MNB: %d\" %round((nltk.classify.accuracy(MNB_classifier, testing_set))*100, 2)\n",
    "print \"BernoulliNB: %d\" %round((nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100, 2)\n",
    "print \"LogisticRegression: %d\" %round((nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100, 2)\n",
    "print \"SGDClassifier: %d\" %round((nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100, 2)\n",
    "print \"LinearSVC: %d\" %round((nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Out of all these models, the one that has the highest accuracy is the Linear SVC model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
