{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 and 6 - Assignment 6\n",
    "#### Team 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Classify new \"test\" documents using already classified \"training\" documents.\n",
    "\n",
    "For this project, the dataset used was from the CrowdFlower library (https://www.crowdflower.com/data-for-everyone/), using the \"Twitter sentiment analysis: Self-driving cars\" dataset. This dataset includes posts from Twitter regarding users' opinions of self-driving cars, the tweets' classification as relevant to the topic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data\n",
    "\n",
    "#### Seeing what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import sys\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>724227037</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.5367</td>\n",
       "      <td>Chicago metro expected to be fully autonomous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>724227038</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>I love the infotainment system in my new car. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id     sentiment  sentiment:confidence  \\\n",
       "0   724227031             5                0.7579   \n",
       "1   724227032             5                0.8775   \n",
       "2   724227033             2                0.6805   \n",
       "3   724227034             2                0.8820   \n",
       "4   724227035             3                1.0000   \n",
       "5   724227036             3                1.0000   \n",
       "6   724227037  not_relevant                0.5367   \n",
       "7   724227038  not_relevant                0.6548   \n",
       "8   724227039             5                0.7187   \n",
       "9   724227040             1                0.6412   \n",
       "10  724227041             3                0.9184   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "6   Chicago metro expected to be fully autonomous ...  \n",
       "7   I love the infotainment system in my new car. ...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.io.parsers.read_csv(\"https://raw.githubusercontent.com/Galanopoulog/DATA620-Assignment-6/master/Self-Driving-Cars.csv\")\n",
    "data = data.drop(data.columns[[1,2,3,4,7,8,9]], axis=1)\n",
    "data.text.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True) #remove ascii characters\n",
    "data.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_unit_id                  int64\n",
      "sentiment                object\n",
      "sentiment:confidence    float64\n",
      "text                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant: 6943\n",
      "Not_Relevant: 213\n",
      "Neutral: 4245\n"
     ]
    }
   ],
   "source": [
    "# Count relevant and irrelevant\n",
    "rel_count = len(data[data.sentiment!=\"not_relevant\"])\n",
    "irrel_count = len(data[data.sentiment==\"not_relevant\"])\n",
    "neut_count = len(data[data.sentiment==\"3\"])\n",
    "\n",
    "print \"Relevant: %d\" %rel_count\n",
    "print \"Not_Relevant: %d\" %irrel_count\n",
    "print \"Neutral: %d\" %neut_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key things to note is that 1) the tweets and sentiment values are objects, which may need to be changed in the future for manipulations and 2) the number of non-relevant tweets is significantly smaller than the relevant portion, so splitting the data into training and test sets should probably be more methodical than just randomly shuffling the data. Also, it will be interesting to see the difference between the relevant neutral tweets and the irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Splitting the data\n",
    "\n",
    "The data is split into the training set, the evaluation set and the final set that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 5009\n",
      "Evaluation set: 1073\n",
      "Testing set: 1074\n",
      "Total: 7156\n"
     ]
    }
   ],
   "source": [
    "# What sizes should our sets be?\n",
    "train_set = int(0.7 * len(data))\n",
    "dev_set = int(.15 * len(data))\n",
    "final_set = len(data) - train_set - dev_set\n",
    "Total = train_set + dev_set + final_set\n",
    "\n",
    "print \"Training set: %d\" %train_set\n",
    "print \"Evaluation set: %d\" %dev_set\n",
    "print \"Testing set: %d\" %final_set\n",
    "print \"Total: %d\" %Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4860\n",
      "Evaluation set: 1041\n",
      "Testing set: 1042\n",
      "Total: 6943\n",
      "----------------------\n",
      "Training set: 149\n",
      "Evaluation set: 31\n",
      "Testing set: 33\n",
      "Total: 213\n"
     ]
    }
   ],
   "source": [
    "# Let's split our data into relevant and non-relevant parts.\n",
    "relevant = data[data.sentiment!=\"not_relevant\"]\n",
    "nonrelevant = data[data.sentiment==\"not_relevant\"]\n",
    "\n",
    "# Let's take 70%, 15% and the 15% of each\n",
    "rel_train = int(0.7 * len(relevant))\n",
    "rel_dev = int(.15 * len(relevant))\n",
    "rel_fin = len(relevant) - rel_train - rel_dev\n",
    "rel_Total = rel_train + rel_dev + rel_fin\n",
    "\n",
    "irrel_train = int(0.7 * len(nonrelevant))\n",
    "irrel_dev = int(.15 * len(nonrelevant))\n",
    "irrel_fin = len(nonrelevant) - irrel_train - irrel_dev\n",
    "irrel_Total = irrel_train + irrel_dev + irrel_fin\n",
    "\n",
    "print \"Training set: %d\" %rel_train\n",
    "print \"Evaluation set: %d\" %rel_dev\n",
    "print \"Testing set: %d\" %rel_fin\n",
    "print \"Total: %d\" %rel_Total\n",
    "print \"----------------------\"\n",
    "print \"Training set: %d\" %irrel_train\n",
    "print \"Evaluation set: %d\" %irrel_dev\n",
    "print \"Testing set: %d\" %irrel_fin\n",
    "print \"Total: %d\" %irrel_Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 5009\n",
      "Evaluation set: 1072\n",
      "Testing set: 1075\n"
     ]
    }
   ],
   "source": [
    "# Make sets for relevant data\n",
    "rel_train = relevant[0:4860]\n",
    "rel_dev = relevant[4860:5901]\n",
    "rel_fin = relevant[5901:6943]\n",
    "\n",
    "# Make sets for non-relevant data\n",
    "irrel_train = nonrelevant[0:149]\n",
    "irrel_dev = nonrelevant[149:180]\n",
    "irrel_fin = nonrelevant[180:231]\n",
    "\n",
    "# Combine sets by set types\n",
    "train_set = pd.concat([rel_train, irrel_train], axis=0)\n",
    "dev_set = pd.concat([rel_dev, irrel_dev], axis=0)\n",
    "final_set = pd.concat([rel_fin, irrel_fin], axis=0)\n",
    "\n",
    "# Determine lengths and compare to set size estimates\n",
    "print \"Training set: %d\" %len(train_set)\n",
    "print \"Evaluation set: %d\" %len(dev_set)\n",
    "print \"Testing set: %d\" %len(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sets now have an evenly distributed number of relevant and non-relevant entries and match the 70%, 15%, 15% data distribution that was considered optimal originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and Categorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>724227610</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Audi is the first carmaker to get a license fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id sentiment  sentiment:confidence  \\\n",
       "0   724227031         5                0.7579   \n",
       "1   724227032         5                0.8775   \n",
       "2   724227033         2                0.6805   \n",
       "3   724227034         2                0.8820   \n",
       "4   724227035         3                1.0000   \n",
       "5   724227036         3                1.0000   \n",
       "8   724227039         5                0.7187   \n",
       "9   724227040         1                0.6412   \n",
       "10  724227041         3                0.9184   \n",
       "11  724227610         3                1.0000   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  \n",
       "11  Audi is the first carmaker to get a license fr...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making tuples for classification\n",
    "subset = train_set[['text', 'sentiment']]\n",
    "tuples = [tuple(x) for x in subset.values]\n",
    "\n",
    "train_set[['text']][0:10]\n",
    "subset[['text']]\n",
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "all_words = toktok.tokenize(subset[['text']] )\n",
    "\n",
    "t = [({word: (word in (x[0])) for word in all_words}, x[1]) for x in tuples]\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    Damn = True                1 : 3      =     41.9 : 1.0\n",
      "                    Fuck = True                1 : 3      =     41.9 : 1.0\n",
      "                    cool = True                5 : 3      =     33.4 : 1.0\n",
      "                 Awesome = True                5 : 3      =     32.7 : 1.0\n",
      "                    seem = True                1 : 3      =     23.3 : 1.0\n",
      "                  reduce = True                5 : 3      =     22.4 : 1.0\n",
      "                       c = False          not_re : 2      =     20.9 : 1.0\n",
      "                tracking = True           not_re : 3      =     20.4 : 1.0\n",
      "         _______________ = True           not_re : 3      =     20.4 : 1.0\n",
      "                     Dor = True           not_re : 3      =     20.4 : 1.0\n",
      "               surprised = True           not_re : 3      =     20.4 : 1.0\n",
      "                 On-Road = True           not_re : 3      =     20.4 : 1.0\n",
      "                  Second = True           not_re : 3      =     20.4 : 1.0\n",
      "                     SAE = True           not_re : 3      =     20.4 : 1.0\n",
      "                    rows = True           not_re : 3      =     20.4 : 1.0\n",
      "                   Chevy = True           not_re : 3      =     20.4 : 1.0\n",
      "              Driverless = True                1 : not_re =     19.9 : 1.0\n",
      "                       e = False          not_re : 4      =     19.4 : 1.0\n",
      "                  places = True                5 : 3      =     16.0 : 1.0\n",
      "                    maps = True           not_re : 4      =     15.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifying Methods\n",
    "\n",
    "# Naive Bayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:\n",
      "------------------\n",
      "Naive Bayes: 52\n",
      "MNB: 60\n",
      "BernoulliNB: 53\n",
      "LogisticRegression: 65\n",
      "SGDClassifier: 49\n",
      "LinearSVC: 65\n"
     ]
    }
   ],
   "source": [
    "# Other Methods to Consider\n",
    "\n",
    "# MNB\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(t)\n",
    "\n",
    "# BernoulliNB\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(t)\n",
    "\n",
    "# Logistic Regression\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(t)\n",
    "\n",
    "# SGDC\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(t)\n",
    "\n",
    "# Linear SVC\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(t)\n",
    "\n",
    "print \"Percent Accuracy:\"\n",
    "print \"------------------\"\n",
    "print \"Naive Bayes: %d\" %round((nltk.classify.accuracy(classifier, t))*100, 2)\n",
    "print \"MNB: %d\" %round((nltk.classify.accuracy(MNB_classifier, t))*100, 2)\n",
    "print \"BernoulliNB: %d\" %round((nltk.classify.accuracy(BernoulliNB_classifier, t))*100, 2)\n",
    "print \"LogisticRegression: %d\" %round((nltk.classify.accuracy(LogisticRegression_classifier, t))*100, 2)\n",
    "print \"SGDClassifier: %d\" %round((nltk.classify.accuracy(SGDClassifier_classifier, t))*100, 2)\n",
    "print \"LinearSVC: %d\" %round((nltk.classify.accuracy(LinearSVC_classifier, t))*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Out of all these models, the one that has the highest accuracy is the Linear SVC model. As interesting as it is to see the differences in each of the values within relevant tweets, it may be best to change the values of relevant (from 5 to 1) to one single value to better see the difference between relevant and non-relevant tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing non-relevant values to \"relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgia\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Georgia\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>724227031</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724227032</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>724227033</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.6805</td>\n",
       "      <td>If Google maps can't keep up with road constru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724227034</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>Autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>724227035</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Just saw Google self-driving car on I-34. It w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>724227036</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Will driverless cars eventually replace taxi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>724227039</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.7187</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>724227040</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.6412</td>\n",
       "      <td>Driverless cars are not worth the risk.  Don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>724227041</td>\n",
       "      <td>relevant</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>Driverless cars are now legal in Florida, Cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>724227610</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Audi is the first carmaker to get a license fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id sentiment  sentiment:confidence  \\\n",
       "0   724227031  relevant                0.7579   \n",
       "1   724227032  relevant                0.8775   \n",
       "2   724227033  relevant                0.6805   \n",
       "3   724227034  relevant                0.8820   \n",
       "4   724227035  relevant                1.0000   \n",
       "5   724227036  relevant                1.0000   \n",
       "8   724227039  relevant                0.7187   \n",
       "9   724227040  relevant                0.6412   \n",
       "10  724227041  relevant                0.9184   \n",
       "11  724227610  relevant                1.0000   \n",
       "\n",
       "                                                 text  \n",
       "0   Two places I'd invest all my money if I could:...  \n",
       "1   Awesome! Google driverless cars will help the ...  \n",
       "2   If Google maps can't keep up with road constru...  \n",
       "3   Autonomous cars seem way overhyped given the t...  \n",
       "4   Just saw Google self-driving car on I-34. It w...  \n",
       "5   Will driverless cars eventually replace taxi d...  \n",
       "8   Autonomous vehicles could reduce traffic fatal...  \n",
       "9   Driverless cars are not worth the risk.  Don't...  \n",
       "10  Driverless cars are now legal in Florida, Cali...  \n",
       "11  Audi is the first carmaker to get a license fr...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing values\n",
    "train_set.sentiment[train_set.sentiment != \"not_relevant\"] = \"relevant\"\n",
    "final_set.sentiment[final_set.sentiment != \"not_relevant\"] = \"relevant\" # testing dataset\n",
    "train_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment:confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>724325793</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Damn the Stingray ! Chevy gots to have the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>724325819</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>Can you guys picture my little 5 foot tall sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>724325839</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>@samuelfine I very rarely drive for any distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>724325999</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>I should add that my new used car is self-driv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5061</th>\n",
       "      <td>724326216</td>\n",
       "      <td>not_relevant</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>I threw a self-driving dart through a cloud th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id     sentiment  sentiment:confidence  \\\n",
       "4638  724325793  not_relevant                1.0000   \n",
       "4664  724325819  not_relevant                0.7708   \n",
       "4684  724325839  not_relevant                0.2480   \n",
       "4844  724325999  not_relevant                1.0000   \n",
       "5061  724326216  not_relevant                0.7653   \n",
       "\n",
       "                                                   text  \n",
       "4638  Damn the Stingray ! Chevy gots to have the bes...  \n",
       "4664  Can you guys picture my little 5 foot tall sel...  \n",
       "4684  @samuelfine I very rarely drive for any distan...  \n",
       "4844  I should add that my new used car is self-driv...  \n",
       "5061  I threw a self-driving dart through a cloud th...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-making tuples\n",
    "subset = train_set[['text', 'sentiment']]\n",
    "tuples = [tuple(x) for x in subset.values]\n",
    "\n",
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "all_words = toktok.tokenize(subset[['text']] )\n",
    "\n",
    "s = [({word: (word in (x[0])) for word in all_words}, x[1]) for x in tuples]\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   threw = True           not_re : releva =     54.0 : 1.0\n",
      "                  Chrome = True           not_re : releva =     32.4 : 1.0\n",
      "                tracking = True           not_re : releva =     32.4 : 1.0\n",
      "         _______________ = True           not_re : releva =     32.4 : 1.0\n",
      "                     Dor = True           not_re : releva =     32.4 : 1.0\n",
      "               surprised = True           not_re : releva =     32.4 : 1.0\n",
      "                  Second = True           not_re : releva =     32.4 : 1.0\n",
      "                   waved = True           not_re : releva =     32.4 : 1.0\n",
      "                     SAE = True           not_re : releva =     32.4 : 1.0\n",
      "                 On-Road = True           not_re : releva =     32.4 : 1.0\n",
      "                   Chevy = True           not_re : releva =     32.4 : 1.0\n",
      "                   view. = True           not_re : releva =     32.4 : 1.0\n",
      "               Standards = True           not_re : releva =     19.4 : 1.0\n",
      "            ____________ = True           not_re : releva =     19.4 : 1.0\n",
      "                    Fuck = True           not_re : releva =     19.4 : 1.0\n",
      "                    dart = True           not_re : releva =     19.4 : 1.0\n",
      "                  rarely = True           not_re : releva =     19.4 : 1.0\n",
      "                    pics = True           not_re : releva =     18.0 : 1.0\n",
      "                     dad = True           not_re : releva =     14.7 : 1.0\n",
      "               #googlema = True           not_re : releva =     12.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(s)\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:\n",
      "------------------\n",
      "Naive Bayes: 91\n",
      "LinearSVC: 97\n"
     ]
    }
   ],
   "source": [
    "print \"Percent Accuracy:\"\n",
    "print \"------------------\"\n",
    "print \"Naive Bayes: %d\" %round((nltk.classify.accuracy(classifier, s))*100, 2)\n",
    "print \"LinearSVC: %d\" %round((nltk.classify.accuracy(LinearSVC_classifier, s))*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the models on the final testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:\n",
      "------------------\n",
      "Naive Bayes: 96\n",
      "LinearSVC: 87\n"
     ]
    }
   ],
   "source": [
    "subset2 = final_set[['text', 'sentiment']]\n",
    "tuples2 = [tuple(x) for x in subset2.values]\n",
    "all_words2 = toktok.tokenize(subset2[['text']])\n",
    "s2 = [({word: (word in (x[0])) for word in all_words2}, x[1]) for x in tuples2]\n",
    "\n",
    "print \"Percent Accuracy:\"\n",
    "print \"------------------\"\n",
    "print \"Naive Bayes: %d\" %round((nltk.classify.accuracy(classifier, s2))*100, 2)\n",
    "print \"LinearSVC: %d\" %round((nltk.classify.accuracy(LinearSVC_classifier, s2))*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The Linear SVC accuracy dropped for the testing dataset, while the Naive Bayes model inccreased. The potential reason for this may be due to how the models work. Naive Bayes relies on classifying text based on each word's probablity while the Linear SVC model determines classification using a \"best fit\" approach to a collection of words. When determining if a topic is not relevant, it may be difficult to classify relevancy due to the abundance and sheer randomness of non-relevant terms. Overall, classifying non-relevant terms as \"not_relevant\" may be harder than determining the classification between two distinct groups simply because something \"not_relevant\" can be completely different from the topic at hand or close enough to the topic to mention it as a tangent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
